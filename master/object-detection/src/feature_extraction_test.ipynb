{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": 21,
         "metadata": {
            "trusted": true
         },
         "outputs": [],
         "source": [
            "import numpy as np\n",
            "import matplotlib.pyplot as plt\n",
            "from sklearn.svm import SVC\n",
            "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
            "from sklearn.model_selection import train_test_split\n",
            "from skimage.feature import hog, local_binary_pattern\n",
            "from skimage.io import imread\n",
            "from skimage.transform import resize\n",
            "from skimage.color import rgb2gray\n",
            "from sklearn.svm import LinearSVC\n",
            "from sklearn.model_selection import train_test_split\n",
            "import matplotlib.pyplot as plt\n",
            "from sklearn.preprocessing import LabelEncoder\n",
            "import pandas as pd\n",
            "from sklearn.ensemble import RandomForestClassifier\n",
            "import os\n",
            "import cv2"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 22,
         "metadata": {},
         "outputs": [],
         "source": [
            "#Initialize Global Variables\n",
            "data = []\n",
            "# data_dir = '../dataset/train_data'\n",
            "data_dir = '../../../notebooks/sushant07/raw_image/test'"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 23,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Tile image to hog features extraction\n",
            "def extract_hog_features(file):\n",
            "    file_path = os.path.join(data_dir, file)\n",
            "    image = imread(file_path)\n",
            "    gray_image = rgb2gray(image)\n",
            "    resized_image = resize(gray_image, (800, 600), anti_aliasing=True)\n",
            "    \n",
            "    # Get image dimensions\n",
            "    height, width = resized_image.shape\n",
            "    \n",
            "    # Calculate tile size\n",
            "    tile_height = height // 8\n",
            "    tile_width = width // 8\n",
            "    \n",
            "    # Create figure for 8x8 subplot\n",
            "    # fig, axes = plt.subplots(8, 8, figsize=(12, 12))\n",
            "    # fig.suptitle(f'Tiles for {file}', fontsize=16)\n",
            "    # Dictionary to store features for this image\n",
            "    hog_features = {}\n",
            "    # Split image into 8x8 tiles\n",
            "    for i in range(8):\n",
            "        for j in range(8):\n",
            "            # Calculate tile boundaries\n",
            "            y_start = i * tile_height\n",
            "            y_end = (i + 1) * tile_height\n",
            "            x_start = j * tile_width\n",
            "            x_end = (j + 1) * tile_width\n",
            "            \n",
            "            # Extract tile\n",
            "            tile = resized_image[y_start:y_end, x_start:x_end]\n",
            "            \n",
            "            # Display tile in subplot\n",
            "            # axes[i, j].imshow(tile, cmap='gray')\n",
            "            # axes[i, j].axis('off')\n",
            "            # axes[i, j].set_title(f'{i},{j}', fontsize=6)\n",
            "            \n",
            "            # Extract HOG features from tile\n",
            "            features = hog(tile, orientations=9,\n",
            "                        pixels_per_cell=(8, 8),\n",
            "                        cells_per_block=(2, 2),\n",
            "                        block_norm='L2-Hys')\n",
            "            # Store in dictionary with key: (image_name, i, j)\n",
            "            key = (file, i, j)\n",
            "            hog_features[key] = features\n",
            "            \n",
            "            # Also append to global data list\n",
            "            data.append({\n",
            "                'type': 'hog',\n",
            "                'image': file,\n",
            "                'tile_i': i,\n",
            "                'tile_j': j,\n",
            "                'features': features\n",
            "            })\n",
            "    # plt.tight_layout()\n",
            "    # plt.show()\n",
            "    return hog_features"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 24,
         "metadata": {},
         "outputs": [],
         "source": [
            "def extract_lbp_features(file):\n",
            "    file_path = os.path.join(data_dir, file)\n",
            "    image = imread(file_path)\n",
            "    gray_image = rgb2gray(image)\n",
            "    resized_image = resize(gray_image, (800, 600), anti_aliasing=True)\n",
            "    # Get image dimensions\n",
            "    height, width = resized_image.shape\n",
            "    # Calculate tile size\n",
            "    tile_height = height // 8\n",
            "    tile_width = width // 8\n",
            "    # Dictionary to store features for this image\n",
            "    lbp_features = {}\n",
            "    # Split image into 8x8 tiles\n",
            "    for i in range(8):\n",
            "        for j in range(8):\n",
            "            # Calculate tile boundaries\n",
            "            y_start = i * tile_height\n",
            "            y_end = (i + 1) * tile_height\n",
            "            x_start = j * tile_width\n",
            "            x_end = (j + 1) * tile_width\n",
            "            \n",
            "            # Extract tile\n",
            "            tile = resized_image[y_start:y_end, x_start:x_end]\n",
            "            tile_uint8 = (tile * 255).astype(np.uint8)\n",
            "            # Extract LBP features from tile\n",
            "            P = 8\n",
            "            R = 1\n",
            "            n_bins = P + 2\n",
            "            lbp = local_binary_pattern(tile_uint8, P=P, R=R, method=\"uniform\")\n",
            "            # lbp = local_binary_pattern(tile, n_points, radius, method='uniform')\n",
            "            (hist, _) = np.histogram(lbp.ravel(),\n",
            "                           bins=n_bins,\n",
            "                           range=(0, n_bins),\n",
            "                           density=True)\n",
            "            hist = hist.astype(\"float\")\n",
            "            hist /= (hist.sum() + 1e-7)\n",
            "            \n",
            "            # Store in dictionary with key: (image_name, i, j)\n",
            "            key = (file, i, j)\n",
            "            lbp_features[key] = hist\n",
            "            \n",
            "            # Also append to global data list\n",
            "            data.append({\n",
            "                'type': 'lbp',\n",
            "                'image': file,\n",
            "                'tile_i': i,\n",
            "                'tile_j': j,\n",
            "                'features': hist\n",
            "            })\n",
            "    # plt.tight_layout()\n",
            "    # plt.show()\n",
            "    return lbp_features"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 25,
         "metadata": {},
         "outputs": [],
         "source": [
            "def plot_color_histogram(file, tile_i, tile_j, tile):\n",
            "    \"\"\"Plot color histograms in 2x2 subplot\"\"\"\n",
            "    fig, axes = plt.subplots(2, 2, figsize=(10, 8))\n",
            "    fig.suptitle(f'{file} - Tile ({tile_i},{tile_j})', fontsize=14)\n",
            "    \n",
            "    # Compute histograms for each channel\n",
            "    hist_r, bins_r = np.histogram(tile[:, :, 0], bins=32, range=(0, 1))\n",
            "    hist_g, bins_g = np.histogram(tile[:, :, 1], bins=32, range=(0, 1))\n",
            "    hist_b, bins_b = np.histogram(tile[:, :, 2], bins=32, range=(0, 1))\n",
            "    \n",
            "    # Top-left: All channels combined\n",
            "    axes[0, 0].plot(bins_r[:-1], hist_r, color='red', alpha=0.7, label='Red')\n",
            "    axes[0, 0].plot(bins_g[:-1], hist_g, color='green', alpha=0.7, label='Green')\n",
            "    axes[0, 0].plot(bins_b[:-1], hist_b, color='blue', alpha=0.7, label='Blue')\n",
            "    axes[0, 0].set_title('All Channels')\n",
            "    axes[0, 0].set_xlabel('Pixel Intensity')\n",
            "    axes[0, 0].set_ylabel('Frequency')\n",
            "    axes[0, 0].legend()\n",
            "    axes[0, 0].grid(True, alpha=0.3)\n",
            "    \n",
            "    # Top-right: Red channel\n",
            "    axes[0, 1].fill_between(bins_r[:-1], hist_r, color='red', alpha=0.7)\n",
            "    axes[0, 1].set_title('Red Channel')\n",
            "    axes[0, 1].set_xlabel('Pixel Intensity')\n",
            "    axes[0, 1].set_ylabel('Frequency')\n",
            "    axes[0, 1].grid(True, alpha=0.3)\n",
            "    \n",
            "    # Bottom-left: Green channel\n",
            "    axes[1, 0].fill_between(bins_g[:-1], hist_g, color='green', alpha=0.7)\n",
            "    axes[1, 0].set_title('Green Channel')\n",
            "    axes[1, 0].set_xlabel('Pixel Intensity')\n",
            "    axes[1, 0].set_ylabel('Frequency')\n",
            "    axes[1, 0].grid(True, alpha=0.3)\n",
            "    \n",
            "    # Bottom-right: Blue channel\n",
            "    axes[1, 1].fill_between(bins_b[:-1], hist_b, color='blue', alpha=0.7)\n",
            "    axes[1, 1].set_title('Blue Channel')\n",
            "    axes[1, 1].set_xlabel('Pixel Intensity')\n",
            "    axes[1, 1].set_ylabel('Frequency')\n",
            "    axes[1, 1].grid(True, alpha=0.3)\n",
            "    \n",
            "    # plt.tight_layout()\n",
            "    # plt.show()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 26,
         "metadata": {},
         "outputs": [],
         "source": [
            "def extract_color_histogram_features(file):\n",
            "    file_path = os.path.join(data_dir, file)\n",
            "    image = imread(file_path)\n",
            "    resized_image = resize(image, (800, 600), anti_aliasing=True)\n",
            "    # Get image dimensions\n",
            "    height, width, _ = resized_image.shape\n",
            "    # Calculate tile size\n",
            "    tile_height = height // 8\n",
            "    tile_width = width // 8\n",
            "    # Dictionary to store features for this image\n",
            "    color_hist_features = {}\n",
            "    # Split image into 8x8 tiles\n",
            "    for i in range(8):\n",
            "        for j in range(8):\n",
            "            # Calculate tile boundaries\n",
            "            y_start = i * tile_height\n",
            "            y_end = (i + 1) * tile_height\n",
            "            x_start = j * tile_width\n",
            "            x_end = (j + 1) * tile_width\n",
            "            \n",
            "            # Extract tile\n",
            "            tile = resized_image[y_start:y_end, x_start:x_end]\n",
            "            \n",
            "            # Compute color histogram for each channel and concatenate\n",
            "            hist_r, _ = np.histogram(tile[:, :, 0], bins=32, range=(0, 1))\n",
            "            hist_g, _ = np.histogram(tile[:, :, 1], bins=32, range=(0, 1))\n",
            "            hist_b, _ = np.histogram(tile[:, :, 2], bins=32, range=(0, 1))\n",
            "            hist = np.concatenate([hist_r, hist_g, hist_b]).astype(\"float\")\n",
            "            hist /= (hist.sum() + 1e-7)\n",
            "            \n",
            "            # Store in dictionary with key: (image_name, i, j)\n",
            "            key = (file, i, j)\n",
            "            color_hist_features[key] = hist\n",
            "            # Plot histogram for first tile only (optional)\n",
            "            if i == 0 and j == 0:\n",
            "                plot_color_histogram(file, i, j, tile)\n",
            "            # Also append to global data list\n",
            "            data.append({\n",
            "                'type': 'color_histogram',\n",
            "                'image': file,\n",
            "                'tile_i': i,\n",
            "                'tile_j': j,\n",
            "                'features': hist\n",
            "            })\n",
            "    return color_hist_features"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 27,
         "metadata": {},
         "outputs": [],
         "source": [
            "def extract_orb_features(file, n_features=500):\n",
            "    \"\"\"\n",
            "    Extract ORB features from image tiles.\n",
            "    ORB is rotation-invariant and efficient for object detection.\n",
            "    \n",
            "    Parameters:\n",
            "    - file: Image filename\n",
            "    - n_features: Number of ORB keypoints to detect (default 500)\n",
            "    \"\"\"\n",
            "    file_path = os.path.join(data_dir, file)\n",
            "    image = imread(file_path)\n",
            "    \n",
            "    # Convert to grayscale for ORB detection\n",
            "    gray_image = rgb2gray(image)\n",
            "    resized_image = resize(gray_image, (800, 600), anti_aliasing=True)\n",
            "    \n",
            "    # Convert to 8-bit format for OpenCV\n",
            "    resized_image_uint8 = (resized_image * 255).astype(np.uint8)\n",
            "    \n",
            "    # Get image dimensions\n",
            "    height, width = resized_image_uint8.shape\n",
            "    \n",
            "    # Calculate tile size\n",
            "    tile_height = height // 8\n",
            "    tile_width = width // 8\n",
            "    \n",
            "    # Initialize ORB detector\n",
            "    orb = cv2.ORB_create(nfeatures=n_features)\n",
            "    \n",
            "    # Dictionary to store ORB features\n",
            "    orb_features = {}\n",
            "    \n",
            "    print(f\"Extracting ORB features from {file}...\")\n",
            "    \n",
            "    # Split image into 8x8 tiles\n",
            "    for i in range(8):\n",
            "        for j in range(8):\n",
            "            # Calculate tile boundaries\n",
            "            y_start = i * tile_height\n",
            "            y_end = (i + 1) * tile_height\n",
            "            x_start = j * tile_width\n",
            "            x_end = (j + 1) * tile_width\n",
            "            \n",
            "            # Extract tile\n",
            "            tile = resized_image_uint8[y_start:y_end, x_start:x_end]\n",
            "            \n",
            "            # Detect keypoints and compute descriptors\n",
            "            keypoints, descriptors = orb.detectAndCompute(tile, None)\n",
            "            \n",
            "            # Handle case where no keypoints are detected\n",
            "            if descriptors is None:\n",
            "                # Create zero vector of expected size\n",
            "                # ORB descriptor is 32 bytes (256 bits)\n",
            "                descriptors = np.zeros((1, 32), dtype=np.uint8)\n",
            "            \n",
            "            # Flatten descriptor to 1D array for storage\n",
            "            if len(keypoints) > 0:\n",
            "                # Create feature vector: [num_keypoints, keypoint_response_mean, descriptor_mean]\n",
            "                num_keypoints = len(keypoints)\n",
            "                keypoint_responses = np.array([kp.response for kp in keypoints])\n",
            "                response_mean = np.mean(keypoint_responses)\n",
            "                response_std = np.std(keypoint_responses) if len(keypoint_responses) > 1 else 0\n",
            "                \n",
            "                # Compute descriptor statistics\n",
            "                descriptor_mean = np.mean(descriptors, axis=0)\n",
            "                descriptor_std = np.std(descriptors, axis=0)\n",
            "                \n",
            "                # Combine all features into single vector\n",
            "                orb_feature_vector = np.concatenate([\n",
            "                    [num_keypoints, response_mean, response_std],\n",
            "                    descriptor_mean,\n",
            "                    descriptor_std\n",
            "                ])\n",
            "            else:\n",
            "                # If no keypoints, create zero vector\n",
            "                orb_feature_vector = np.zeros(67)  # 3 + 32 + 32\n",
            "            \n",
            "            # Store in dictionary\n",
            "            key = (file, i, j)\n",
            "            orb_features[key] = orb_feature_vector\n",
            "            \n",
            "            # Append to global data list\n",
            "            data.append({\n",
            "                'type': 'orb',\n",
            "                'image': file,\n",
            "                'tile_i': i,\n",
            "                'tile_j': j,\n",
            "                'num_keypoints': len(keypoints),\n",
            "                'features': orb_feature_vector\n",
            "            })\n",
            "    \n",
            "    print(f\"Completed ORB extraction for {file}\")\n",
            "    return orb_features\n",
            "\n",
            "\n",
            "def visualize_orb_features(file, tile_i, tile_j, n_features=500):\n",
            "    \"\"\"\n",
            "    Visualize ORB keypoints on a specific tile\n",
            "    \"\"\"\n",
            "    file_path = os.path.join(data_dir, file)\n",
            "    image = imread(file_path)\n",
            "    gray_image = rgb2gray(image)\n",
            "    resized_image = resize(gray_image, (800, 600), anti_aliasing=True)\n",
            "    resized_image_uint8 = (resized_image * 255).astype(np.uint8)\n",
            "    \n",
            "    height, width = resized_image_uint8.shape\n",
            "    tile_height = height // 8\n",
            "    tile_width = width // 8\n",
            "    \n",
            "    # Extract specific tile\n",
            "    y_start = tile_i * tile_height\n",
            "    y_end = (tile_i + 1) * tile_height\n",
            "    x_start = tile_j * tile_width\n",
            "    x_end = (tile_j + 1) * tile_width\n",
            "    \n",
            "    tile = resized_image_uint8[y_start:y_end, x_start:x_end]\n",
            "    \n",
            "    # Detect ORB features\n",
            "    orb = cv2.ORB_create(nfeatures=n_features)\n",
            "    keypoints, descriptors = orb.detectAndCompute(tile, None)\n",
            "    \n",
            "    # Draw keypoints\n",
            "    tile_with_keypoints = cv2.drawKeypoints(tile, keypoints, None, \n",
            "                                           color=(0, 255, 0),\n",
            "                                           flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
            "    \n",
            "    # Display\n",
            "    plt.figure(figsize=(10, 8))\n",
            "    plt.imshow(tile_with_keypoints, cmap='gray')\n",
            "    plt.title(f'{file} - Tile ({tile_i},{tile_j}) - ORB Keypoints: {len(keypoints)}')\n",
            "    plt.axis('off')\n",
            "    plt.show()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 28,
         "metadata": {},
         "outputs": [],
         "source": [
            "def write_features_to_file(filename='features_data_test_pexels.csv'):\n",
            "    # Group data by (image, tile_i, tile_j)\n",
            "    grouped_data = {}\n",
            "    \n",
            "    for item in data:\n",
            "        key = (item['image'], item['tile_i'], item['tile_j'])\n",
            "        if key not in grouped_data:\n",
            "            grouped_data[key] = {\n",
            "                'image': item['image'],\n",
            "                'tile_i': item['tile_i'],\n",
            "                'tile_j': item['tile_j'],\n",
            "                'hog': None,\n",
            "                'lbp': None,\n",
            "                'orb': None\n",
            "            }\n",
            "        \n",
            "        # Store features by type\n",
            "        grouped_data[key][item['type']] = item['features']\n",
            "    \n",
            "    # Create records with combined features\n",
            "    records = []\n",
            "    for key, tile_data in grouped_data.items():\n",
            "        record = {\n",
            "            'image': tile_data['image'],\n",
            "            'tile_i': tile_data['tile_i'],\n",
            "            'tile_j': tile_data['tile_j']\n",
            "        }\n",
            "        \n",
            "        # Add HOG features\n",
            "        if tile_data['hog'] is not None:\n",
            "            for idx, val in enumerate(tile_data['hog']):\n",
            "                record[f'x_hog_{idx}'] = val\n",
            "        \n",
            "        # Add LBP features\n",
            "        if tile_data['lbp'] is not None:\n",
            "            for idx, val in enumerate(tile_data['lbp']):\n",
            "                record[f'x_lbp_{idx}'] = val\n",
            "        \n",
            "        # Add Color Histogram features\n",
            "        if tile_data['orb'] is not None:\n",
            "            for idx, val in enumerate(tile_data['orb']):\n",
            "                record[f'x_orb_{idx}'] = val\n",
            "        \n",
            "        records.append(record)\n",
            "    \n",
            "    # Create DataFrame\n",
            "    df = pd.DataFrame(records)\n",
            "    \n",
            "    # Save to CSV\n",
            "    df.to_csv(filename, index=False)\n",
            "    \n",
            "    print(f'Saved {len(df)} records to {filename}')\n",
            "    print(f'Shape: {df.shape}')\n",
            "    print(f'Columns: {df.columns[:10].tolist()}...')  # Show first 10 columns"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 29,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Extracted HOG features for pexels.jpg, total tiles: 64 and size of each tile feature vector: 3168\n",
                  "Extracted LBP features for pexels.jpg, total tiles: 64 and size of each tile feature vector: 10\n",
                  "Saved 64 records to features_data_test_pexels.csv\n",
                  "Shape: (64, 3181)\n",
                  "Columns: ['image', 'tile_i', 'tile_j', 'x_hog_0', 'x_hog_1', 'x_hog_2', 'x_hog_3', 'x_hog_4', 'x_hog_5', 'x_hog_6']...\n"
               ]
            }
         ],
         "source": [
            "#Run the extraction\n",
            "for file in os.listdir(data_dir):\n",
            "    hog_features = extract_hog_features(file)\n",
            "    print(f'Extracted HOG features for {file}, total tiles: {len(hog_features)} and size of each tile feature vector: {len(next(iter(hog_features.values())))}')\n",
            "    lbp_features = extract_lbp_features(file)\n",
            "    print(f'Extracted LBP features for {file}, total tiles: {len(lbp_features)} and size of each tile feature vector: {len(next(iter(lbp_features.values())))}')\n",
            "    # Extract ORB features (NEW)\n",
            "    # orb_features = extract_orb_features(file, n_features=500)\n",
            "    # print(f'Extracted ORB features for {file}: {len(orb_features)} tiles')\n",
            "\n",
            "write_features_to_file()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 30,
         "metadata": {},
         "outputs": [],
         "source": [
            "# # Read data from features_data_grp_1.csv\n",
            "# import pandas as pd\n",
            "# df_labelled = pd.read_csv('../outputs/features_data_labelled.csv')\n",
            "\n",
            "# # print(df.head())\n",
            "# print(df_labelled.shape)\n",
            "\n",
            "# #drop columns image, tile_i, tile_j\n",
            "# df_features = df_labelled.drop(columns=['image', 'tile_i', 'tile_j','y'])\n",
            "\n",
            "# # find out correlations coefficients of all features with each other\n",
            "# correlation_matrix = df_features.corr()\n",
            "\n",
            "# #find out features which have correlation coefficient > 0.90 with each other\n",
            "# high_correlation_pairs = []\n",
            "# for i in range(len(correlation_matrix.columns)):\n",
            "#     for j in range(i):\n",
            "#         if abs(correlation_matrix.iloc[i, j]) > 0.90:\n",
            "#             high_correlation_pairs.append((correlation_matrix.columns[i], correlation_matrix.columns[j], correlation_matrix.iloc[i, j]))\n",
            "# print(f'Number of highly correlated feature pairs (>0.90): {len(high_correlation_pairs)}')\n",
            "# #print first 10 highly correlated feature pairs\n",
            "# for pair in high_correlation_pairs[:10]:\n",
            "#     print(pair)\n",
            "\n",
            "# # find unique features from the highly correlated pairs\n",
            "# unique_features_to_drop = set()\n",
            "# for feat1, feat2, corr in high_correlation_pairs:\n",
            "#     unique_features_to_drop.add(feat2)  # arbitrarily drop the second feature in the pair\n",
            "# print(f'Number of unique features to drop due to high correlation: {len(unique_features_to_drop)}')\n",
            "\n",
            "# #drop these features from df_features\n",
            "# df_reduced = df_labelled.drop(columns=list(unique_features_to_drop))\n",
            "# print(f'Shape of features after dropping highly correlated ones: {df_reduced.shape}')\n",
            "\n",
            "# #create csv of reduced features\n",
            "# df_reduced.to_csv('../outputs/features_data_reduced.csv', index=False)"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "base",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.13.5"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 4
}
