{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eec501cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CRICKET OBJECT DETECTION - MODEL TESTING\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "EXAMPLE 1: Testing with Ground Truth Labels\n",
      "======================================================================\n",
      "Loading models and preprocessors...\n",
      "✓ Loaded model: SVC\n",
      "✓ Loaded scaler: RobustScaler\n",
      "✓ Loaded PCA: 109 components\n",
      "✓ Loaded feature selector\n",
      "\n",
      "======================================================================\n",
      "Model Testing Suite Ready!\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "STARTING TEST PIPELINE\n",
      "======================================================================\n",
      "Loading test data from: enhanced_features_pexels_5.csv\n",
      "✓ Test data shape: (256, 3290)\n",
      "✓ Has ground truth labels: False\n",
      "\n",
      "Applying preprocessing...\n",
      "✓ Applied hybrid feature selection\n",
      "  Features after selection: 250\n",
      "✓ Applied scaling\n",
      "✓ Applied PCA: 109 components\n",
      "\n",
      "Making predictions...\n",
      "✓ Predictions complete\n",
      "  Predicted classes: [0 1 2 3]\n",
      "\n",
      "✓ Predictions saved to ../outputs/test_results\\predictions_pexels_5.csv\n",
      "\n",
      "⚠️  No ground truth labels available - skipping evaluation\n",
      "   Only predictions have been saved.\n",
      "\n",
      "======================================================================\n",
      "TEST PIPELINE COMPLETE\n",
      "======================================================================\n",
      "\n",
      "✓ Testing complete! Check outputs in ../outputs/test_results/\n"
     ]
    }
   ],
   "source": [
    "# model_testing.ipynb\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, \n",
    "                            accuracy_score, precision_recall_fscore_support,\n",
    "                            roc_curve, auc, roc_auc_score)\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import pickle\n",
    "import os\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class CricketModelTester:\n",
    "    \"\"\"\n",
    "    Comprehensive testing suite for cricket object detection model.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_path, scaler_path, pca_path=None, feature_selector_path=None):\n",
    "        \"\"\"\n",
    "        Load trained model and preprocessors.\n",
    "        \"\"\"\n",
    "        print(\"Loading models and preprocessors...\")\n",
    "        \n",
    "        # Load model\n",
    "        with open(model_path, 'rb') as f:\n",
    "            self.model = pickle.load(f)\n",
    "        print(f\"✓ Loaded model: {type(self.model).__name__}\")\n",
    "        \n",
    "        # Load scaler\n",
    "        with open(scaler_path, 'rb') as f:\n",
    "            self.scaler = pickle.load(f)\n",
    "        print(f\"✓ Loaded scaler: {type(self.scaler).__name__}\")\n",
    "        \n",
    "        # Load PCA (optional)\n",
    "        self.pca = None\n",
    "        if pca_path and os.path.exists(pca_path):\n",
    "            with open(pca_path, 'rb') as f:\n",
    "                self.pca = pickle.load(f)\n",
    "            print(f\"✓ Loaded PCA: {self.pca.n_components_} components\")\n",
    "        \n",
    "        # Load feature selector (optional)\n",
    "        self.feature_selector = None\n",
    "        if feature_selector_path and os.path.exists(feature_selector_path):\n",
    "            with open(feature_selector_path, 'rb') as f:\n",
    "                self.feature_selector = pickle.load(f)\n",
    "            print(f\"✓ Loaded feature selector\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"Model Testing Suite Ready!\")\n",
    "        print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    def load_test_data(self, test_file_path):\n",
    "        \"\"\"\n",
    "        Load and prepare test data.\n",
    "        \"\"\"\n",
    "        print(f\"Loading test data from: {test_file_path}\")\n",
    "        \n",
    "        df_test = pd.read_csv(test_file_path)\n",
    "        df_clean = df_test.dropna()\n",
    "        \n",
    "        # Separate metadata and features\n",
    "        metadata_cols = ['image', 'tile_i', 'tile_j', 'tile_number', 'augmentation']\n",
    "        metadata_cols = [col for col in metadata_cols if col in df_clean.columns]\n",
    "        \n",
    "        self.metadata = df_clean[metadata_cols].copy()\n",
    "        \n",
    "        # Check if labels exist\n",
    "        if 'y' in df_clean.columns:\n",
    "            self.y_true = df_clean['y'].values\n",
    "            X = df_clean.drop(metadata_cols + ['y'], axis=1)\n",
    "            has_labels = True\n",
    "        else:\n",
    "            self.y_true = None\n",
    "            X = df_clean.drop(metadata_cols, axis=1)\n",
    "            has_labels = False\n",
    "        \n",
    "        print(f\"✓ Test data shape: {X.shape}\")\n",
    "        print(f\"✓ Has ground truth labels: {has_labels}\")\n",
    "        \n",
    "        return X, has_labels\n",
    "    \n",
    "    def preprocess_features(self, X):\n",
    "        \"\"\"\n",
    "        Apply same preprocessing as training.\n",
    "        \"\"\"\n",
    "        print(\"\\nApplying preprocessing...\")\n",
    "        \n",
    "        # Feature selection\n",
    "        if self.feature_selector:\n",
    "            if isinstance(self.feature_selector, tuple):\n",
    "                # Hybrid selector\n",
    "                X = self.feature_selector[0].transform(X)\n",
    "                X = self.feature_selector[1].transform(X)\n",
    "                print(f\"✓ Applied hybrid feature selection\")\n",
    "            else:\n",
    "                X = self.feature_selector.transform(X)\n",
    "                print(f\"✓ Applied feature selection\")\n",
    "            print(f\"  Features after selection: {X.shape[1]}\")\n",
    "        \n",
    "        # Scaling\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        print(f\"✓ Applied scaling\")\n",
    "        \n",
    "        # PCA\n",
    "        if self.pca:\n",
    "            X_final = self.pca.transform(X_scaled)\n",
    "            print(f\"✓ Applied PCA: {X_final.shape[1]} components\")\n",
    "        else:\n",
    "            X_final = X_scaled\n",
    "        \n",
    "        return X_final\n",
    "    \n",
    "    def predict(self, X_preprocessed):\n",
    "        \"\"\"\n",
    "        Make predictions on preprocessed data.\n",
    "        \"\"\"\n",
    "        print(\"\\nMaking predictions...\")\n",
    "        \n",
    "        # Predictions\n",
    "        y_pred = self.model.predict(X_preprocessed)\n",
    "        \n",
    "        # Probabilities\n",
    "        if hasattr(self.model, 'predict_proba'):\n",
    "            y_proba = self.model.predict_proba(X_preprocessed)\n",
    "        else:\n",
    "            y_proba = None\n",
    "        \n",
    "        print(f\"✓ Predictions complete\")\n",
    "        print(f\"  Predicted classes: {np.unique(y_pred)}\")\n",
    "        \n",
    "        return y_pred, y_proba\n",
    "    \n",
    "    def evaluate_performance(self, y_true, y_pred, y_proba=None):\n",
    "        \"\"\"\n",
    "        Comprehensive performance evaluation.\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"PERFORMANCE EVALUATION\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Overall metrics\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        print(f\"\\nOverall Accuracy: {accuracy:.4f}\")\n",
    "        \n",
    "        # Per-class metrics\n",
    "        print(\"\\n\" + \"-\"*70)\n",
    "        print(\"CLASSIFICATION REPORT\")\n",
    "        print(\"-\"*70)\n",
    "        print(classification_report(y_true, y_pred, \n",
    "                                   target_names=[f'Class {i}' for i in np.unique(y_true)]))\n",
    "        \n",
    "        # Confusion Matrix\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        print(\"\\n\" + \"-\"*70)\n",
    "        print(\"CONFUSION MATRIX\")\n",
    "        print(\"-\"*70)\n",
    "        print(cm)\n",
    "        \n",
    "        # Class distribution\n",
    "        print(\"\\n\" + \"-\"*70)\n",
    "        print(\"CLASS DISTRIBUTION\")\n",
    "        print(\"-\"*70)\n",
    "        print(\"True labels:\")\n",
    "        print(pd.Series(y_true).value_counts().sort_index())\n",
    "        print(\"\\nPredicted labels:\")\n",
    "        print(pd.Series(y_pred).value_counts().sort_index())\n",
    "        \n",
    "        # ROC-AUC (if probabilities available)\n",
    "        if y_proba is not None and len(np.unique(y_true)) > 1:\n",
    "            print(\"\\n\" + \"-\"*70)\n",
    "            print(\"ROC-AUC SCORES\")\n",
    "            print(\"-\"*70)\n",
    "            try:\n",
    "                if len(np.unique(y_true)) == 2:\n",
    "                    # Binary classification\n",
    "                    roc_auc = roc_auc_score(y_true, y_proba[:, 1])\n",
    "                    print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
    "                else:\n",
    "                    # Multi-class\n",
    "                    roc_auc = roc_auc_score(y_true, y_proba, \n",
    "                                           multi_class='ovr', average='weighted')\n",
    "                    print(f\"ROC-AUC (weighted): {roc_auc:.4f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Could not compute ROC-AUC: {e}\")\n",
    "        \n",
    "        print(\"=\"*70 + \"\\n\")\n",
    "        \n",
    "        return accuracy, cm\n",
    "    \n",
    "    def visualize_confusion_matrix(self, cm, class_names=None, save_path=None):\n",
    "        \"\"\"\n",
    "        Plot confusion matrix with nice formatting.\n",
    "        \"\"\"\n",
    "        if class_names is None:\n",
    "            class_names = [f'Class {i}' for i in range(len(cm))]\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        \n",
    "        # Normalize confusion matrix\n",
    "        cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        \n",
    "        # Plot\n",
    "        sns.heatmap(cm_norm, annot=True, fmt='.2%', cmap='Blues',\n",
    "                   xticklabels=class_names, yticklabels=class_names,\n",
    "                   cbar_kws={'label': 'Percentage'})\n",
    "        \n",
    "        plt.title('Confusion Matrix (Normalized)', fontsize=16, fontweight='bold')\n",
    "        plt.ylabel('True Label', fontsize=12)\n",
    "        plt.xlabel('Predicted Label', fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"✓ Confusion matrix saved to {save_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        # Also plot counts\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Greens',\n",
    "                   xticklabels=class_names, yticklabels=class_names,\n",
    "                   cbar_kws={'label': 'Count'})\n",
    "        \n",
    "        plt.title('Confusion Matrix (Counts)', fontsize=16, fontweight='bold')\n",
    "        plt.ylabel('True Label', fontsize=12)\n",
    "        plt.xlabel('Predicted Label', fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            count_path = save_path.replace('.png', '_counts.png')\n",
    "            plt.savefig(count_path, dpi=300, bbox_inches='tight')\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    def visualize_roc_curves(self, y_true, y_proba, save_path=None):\n",
    "        \"\"\"\n",
    "        Plot ROC curves for all classes.\n",
    "        \"\"\"\n",
    "        n_classes = y_proba.shape[1]\n",
    "        \n",
    "        # Binarize labels\n",
    "        y_true_bin = label_binarize(y_true, classes=range(n_classes))\n",
    "        \n",
    "        # Compute ROC curve and AUC for each class\n",
    "        fpr = dict()\n",
    "        tpr = dict()\n",
    "        roc_auc = dict()\n",
    "        \n",
    "        for i in range(n_classes):\n",
    "            fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_proba[:, i])\n",
    "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "        \n",
    "        # Plot\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        \n",
    "        colors = ['blue', 'red', 'green', 'orange', 'purple', 'brown', 'pink', 'gray']\n",
    "        for i in range(n_classes):\n",
    "            plt.plot(fpr[i], tpr[i], color=colors[i % len(colors)],\n",
    "                    lw=2, label=f'Class {i} (AUC = {roc_auc[i]:.3f})')\n",
    "        \n",
    "        plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Classifier')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate', fontsize=12)\n",
    "        plt.ylabel('True Positive Rate', fontsize=12)\n",
    "        plt.title('ROC Curves - Multi-Class', fontsize=16, fontweight='bold')\n",
    "        plt.legend(loc='lower right', fontsize=10)\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"✓ ROC curves saved to {save_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    def visualize_prediction_distribution(self, y_true, y_pred, save_path=None):\n",
    "        \"\"\"\n",
    "        Visualize prediction distribution comparison.\n",
    "        \"\"\"\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # True distribution\n",
    "        true_counts = pd.Series(y_true).value_counts().sort_index()\n",
    "        axes[0].bar(true_counts.index.astype(str), true_counts.values, \n",
    "                   color='skyblue', edgecolor='black')\n",
    "        axes[0].set_title('True Label Distribution', fontsize=14, fontweight='bold')\n",
    "        axes[0].set_xlabel('Class', fontsize=12)\n",
    "        axes[0].set_ylabel('Count', fontsize=12)\n",
    "        axes[0].grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        for i, v in enumerate(true_counts.values):\n",
    "            axes[0].text(i, v + max(true_counts.values)*0.01, str(v), \n",
    "                        ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        # Predicted distribution\n",
    "        pred_counts = pd.Series(y_pred).value_counts().sort_index()\n",
    "        axes[1].bar(pred_counts.index.astype(str), pred_counts.values, \n",
    "                   color='lightcoral', edgecolor='black')\n",
    "        axes[1].set_title('Predicted Label Distribution', fontsize=14, fontweight='bold')\n",
    "        axes[1].set_xlabel('Class', fontsize=12)\n",
    "        axes[1].set_ylabel('Count', fontsize=12)\n",
    "        axes[1].grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        for i, v in enumerate(pred_counts.values):\n",
    "            axes[1].text(i, v + max(pred_counts.values)*0.01, str(v), \n",
    "                        ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"✓ Distribution plot saved to {save_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    def per_class_analysis(self, y_true, y_pred, class_names=None):\n",
    "        \"\"\"\n",
    "        Detailed per-class analysis.\n",
    "        \"\"\"\n",
    "        if class_names is None:\n",
    "            class_names = [f'Class {i}' for i in np.unique(y_true)]\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"PER-CLASS DETAILED ANALYSIS\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Get precision, recall, f1 for each class\n",
    "        precision, recall, f1, support = precision_recall_fscore_support(\n",
    "            y_true, y_pred, average=None\n",
    "        )\n",
    "        \n",
    "        # Create DataFrame\n",
    "        results_df = pd.DataFrame({\n",
    "            'Class': class_names,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1-Score': f1,\n",
    "            'Support': support\n",
    "        })\n",
    "        \n",
    "        print(\"\\n\", results_df.to_string(index=False))\n",
    "        \n",
    "        # Visualize\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "        \n",
    "        x = np.arange(len(class_names))\n",
    "        width = 0.6\n",
    "        \n",
    "        # Precision\n",
    "        axes[0].bar(x, precision, width, label='Precision', color='skyblue', edgecolor='black')\n",
    "        axes[0].set_ylabel('Score', fontsize=12)\n",
    "        axes[0].set_title('Precision by Class', fontsize=14, fontweight='bold')\n",
    "        axes[0].set_xticks(x)\n",
    "        axes[0].set_xticklabels(class_names, rotation=45, ha='right')\n",
    "        axes[0].set_ylim([0, 1.1])\n",
    "        axes[0].grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        for i, v in enumerate(precision):\n",
    "            axes[0].text(i, v + 0.02, f'{v:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        # Recall\n",
    "        axes[1].bar(x, recall, width, label='Recall', color='lightcoral', edgecolor='black')\n",
    "        axes[1].set_ylabel('Score', fontsize=12)\n",
    "        axes[1].set_title('Recall by Class', fontsize=14, fontweight='bold')\n",
    "        axes[1].set_xticks(x)\n",
    "        axes[1].set_xticklabels(class_names, rotation=45, ha='right')\n",
    "        axes[1].set_ylim([0, 1.1])\n",
    "        axes[1].grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        for i, v in enumerate(recall):\n",
    "            axes[1].text(i, v + 0.02, f'{v:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        # F1-Score\n",
    "        axes[2].bar(x, f1, width, label='F1-Score', color='lightgreen', edgecolor='black')\n",
    "        axes[2].set_ylabel('Score', fontsize=12)\n",
    "        axes[2].set_title('F1-Score by Class', fontsize=14, fontweight='bold')\n",
    "        axes[2].set_xticks(x)\n",
    "        axes[2].set_xticklabels(class_names, rotation=45, ha='right')\n",
    "        axes[2].set_ylim([0, 1.1])\n",
    "        axes[2].grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        for i, v in enumerate(f1):\n",
    "            axes[2].text(i, v + 0.02, f'{v:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return results_df\n",
    "    \n",
    "    def analyze_misclassifications(self, y_true, y_pred, top_n=10):\n",
    "        \"\"\"\n",
    "        Analyze which classes are most often confused.\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"MISCLASSIFICATION ANALYSIS\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Find misclassified samples\n",
    "        misclassified_mask = y_true != y_pred\n",
    "        n_misclassified = np.sum(misclassified_mask)\n",
    "        \n",
    "        print(f\"\\nTotal misclassified samples: {n_misclassified} ({n_misclassified/len(y_true)*100:.2f}%)\")\n",
    "        \n",
    "        # Confusion pairs\n",
    "        confusion_pairs = []\n",
    "        for true_label, pred_label in zip(y_true[misclassified_mask], \n",
    "                                         y_pred[misclassified_mask]):\n",
    "            confusion_pairs.append((true_label, pred_label))\n",
    "        \n",
    "        # Count confusion pairs\n",
    "        from collections import Counter\n",
    "        pair_counts = Counter(confusion_pairs)\n",
    "        \n",
    "        print(f\"\\nTop {top_n} most common misclassifications:\")\n",
    "        print(\"-\" * 70)\n",
    "        print(f\"{'True Class':<12} {'Predicted Class':<18} {'Count':<10} {'% of Errors'}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        for (true_cls, pred_cls), count in pair_counts.most_common(top_n):\n",
    "            percentage = count / n_misclassified * 100\n",
    "            print(f\"{true_cls:<12} {pred_cls:<18} {count:<10} {percentage:>6.2f}%\")\n",
    "        \n",
    "        print(\"=\"*70)\n",
    "    \n",
    "    def save_predictions(self, y_pred, y_proba=None, output_path='test_predictions.csv'):\n",
    "        \"\"\"\n",
    "        Save predictions to CSV file.\n",
    "        \"\"\"\n",
    "        results_df = self.metadata.copy()\n",
    "        results_df['prediction'] = y_pred\n",
    "        \n",
    "        if self.y_true is not None:\n",
    "            results_df['true_label'] = self.y_true\n",
    "            results_df['correct'] = (self.y_true == y_pred)\n",
    "        \n",
    "        if y_proba is not None:\n",
    "            # Add probability for each class\n",
    "            for i in range(y_proba.shape[1]):\n",
    "                results_df[f'prob_class_{i}'] = y_proba[:, i]\n",
    "            \n",
    "            # Add confidence (max probability)\n",
    "            results_df['confidence'] = np.max(y_proba, axis=1)\n",
    "        \n",
    "        results_df.to_csv(output_path, index=False)\n",
    "        print(f\"\\n✓ Predictions saved to {output_path}\")\n",
    "        \n",
    "        return results_df\n",
    "    \n",
    "    def test_pipeline(self, test_file_path, output_dir='../outputs/test_results',\n",
    "                     class_names=None, visualize=True):\n",
    "        \"\"\"\n",
    "        Complete testing pipeline.\n",
    "        \"\"\"\n",
    "        # Create output directory\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        print(\"=\"*70)\n",
    "        print(\"STARTING TEST PIPELINE\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Load test data\n",
    "        X_test, has_labels = self.load_test_data(test_file_path)\n",
    "        \n",
    "        # Preprocess\n",
    "        X_preprocessed = self.preprocess_features(X_test)\n",
    "        \n",
    "        # Predict\n",
    "        y_pred, y_proba = self.predict(X_preprocessed)\n",
    "        \n",
    "        # Save predictions\n",
    "        results_df = self.save_predictions(\n",
    "            y_pred, y_proba, \n",
    "            output_path=os.path.join(output_dir, 'predictions_pexels_5.csv')\n",
    "        )\n",
    "        \n",
    "        # If ground truth available, evaluate\n",
    "        if has_labels and self.y_true is not None:\n",
    "            # Evaluate\n",
    "            accuracy, cm = self.evaluate_performance(self.y_true, y_pred, y_proba)\n",
    "            \n",
    "            if visualize:\n",
    "                # Confusion matrix\n",
    "                self.visualize_confusion_matrix(\n",
    "                    cm, class_names=class_names,\n",
    "                    save_path=os.path.join(output_dir, 'confusion_matrix.png')\n",
    "                )\n",
    "                \n",
    "                # ROC curves\n",
    "                if y_proba is not None:\n",
    "                    self.visualize_roc_curves(\n",
    "                        self.y_true, y_proba,\n",
    "                        save_path=os.path.join(output_dir, 'roc_curves.png')\n",
    "                    )\n",
    "                \n",
    "                # Distribution comparison\n",
    "                self.visualize_prediction_distribution(\n",
    "                    self.y_true, y_pred,\n",
    "                    save_path=os.path.join(output_dir, 'distribution.png')\n",
    "                )\n",
    "                \n",
    "                # Per-class analysis\n",
    "                per_class_df = self.per_class_analysis(self.y_true, y_pred, class_names)\n",
    "                per_class_df.to_csv(\n",
    "                    os.path.join(output_dir, 'per_class_metrics.csv'), \n",
    "                    index=False\n",
    "                )\n",
    "                \n",
    "                # Misclassification analysis\n",
    "                self.analyze_misclassifications(self.y_true, y_pred)\n",
    "        else:\n",
    "            print(\"\\n⚠️  No ground truth labels available - skipping evaluation\")\n",
    "            print(\"   Only predictions have been saved.\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"TEST PIPELINE COMPLETE\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        return results_df\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# USAGE EXAMPLES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CRICKET OBJECT DETECTION - MODEL TESTING\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Example 1: Test with ground truth labels\n",
    "print(\"=\"*70)\n",
    "print(\"EXAMPLE 1: Testing with Ground Truth Labels\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "tester = CricketModelTester(\n",
    "    model_path='../models/best_model.pkl',\n",
    "    scaler_path='../models/scaler.pkl',\n",
    "    pca_path='../models/pca_model.pkl',\n",
    "    feature_selector_path='../models/feature_selector.pkl'\n",
    ")\n",
    "\n",
    "# Define class names (customize based on your labels)\n",
    "class_names = ['Background', 'Ball', 'Bat', 'Stadium']\n",
    "\n",
    "# Run test pipeline\n",
    "results = tester.test_pipeline(\n",
    "    test_file_path='enhanced_features_pexels_5.csv',\n",
    "    output_dir='../outputs/test_results',\n",
    "    class_names=class_names,\n",
    "    visualize=True\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Testing complete! Check outputs in ../outputs/test_results/\")\n",
    "\n",
    "# Example 2: Test without ground truth (inference only)\n",
    "# print(\"\\n\" + \"=\"*70)\n",
    "# print(\"EXAMPLE 2: Inference on Unlabeled Data\")\n",
    "# print(\"=\"*70)\n",
    "\n",
    "# tester2 = CricketModelTester(\n",
    "#     model_path='../models/best_model.pkl',\n",
    "#     scaler_path='../models/scaler.pkl',\n",
    "#     pca_path='../models/pca_model.pkl',\n",
    "#     feature_selector_path='../models/feature_selector.pkl'\n",
    "# )\n",
    "\n",
    "# results_inference = tester2.test_pipeline(\n",
    "#     test_file_path='unlabeled_features.csv',\n",
    "#     output_dir='../outputs/inference_results',\n",
    "#     visualize=False\n",
    "# )\n",
    "\n",
    "# print(\"\\n✓ Inference complete! Check predictions in ../outputs/inference_results/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
