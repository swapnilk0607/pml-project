{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6b97a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (22528, 3290)\n",
      "Class distribution:\n",
      "y\n",
      "0    19859\n",
      "3     1289\n",
      "2     1103\n",
      "1      277\n",
      "Name: count, dtype: int64\n",
      "✓ Selected 250 features using hybrid selection\n",
      "\n",
      "Original distribution: {np.int64(0): np.int64(19859), np.int64(1): np.int64(277), np.int64(2): np.int64(1103), np.int64(3): np.int64(1289)}\n",
      "After undersampling: {np.int64(0): np.int64(2669), np.int64(1): np.int64(277), np.int64(2): np.int64(1103), np.int64(3): np.int64(1289)}\n",
      "After SMOTE: {np.int64(0): np.int64(2669), np.int64(1): np.int64(2669), np.int64(2): np.int64(2669), np.int64(3): np.int64(2669)}\n",
      "PCA: 109 components (95% variance)\n",
      "\n",
      "======================================================================\n",
      "Training RandomForest...\n",
      "======================================================================\n",
      "CV F1 Scores: [0.82895578 0.85647438 0.83680075 0.84519389 0.82231233]\n",
      "Mean CV F1: 0.8379 (+/- 0.0120)\n",
      "\n",
      "======================================================================\n",
      "Training SVC-RBF...\n",
      "======================================================================\n",
      "CV F1 Scores: [0.86817587 0.8862951  0.87772156 0.87129083 0.86920871]\n",
      "Mean CV F1: 0.8745 (+/- 0.0067)\n",
      "\n",
      "======================================================================\n",
      "Best Model: SVC-RBF\n",
      "CV F1 Score: 0.8745\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "TEST SET EVALUATION\n",
      "======================================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.78      0.81       534\n",
      "           1       0.98      0.97      0.97       534\n",
      "           2       0.86      0.90      0.88       534\n",
      "           3       0.89      0.92      0.91       534\n",
      "\n",
      "    accuracy                           0.89      2136\n",
      "   macro avg       0.89      0.89      0.89      2136\n",
      "weighted avg       0.89      0.89      0.89      2136\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[419   8  63  44]\n",
      " [  6 517   9   2]\n",
      " [ 39   2 481  12]\n",
      " [ 31   3   9 491]]\n",
      "\n",
      "✓ Models saved (Best: SVC-RBF)\n"
     ]
    }
   ],
   "source": [
    "# enhanced_model_training.ipynb\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTENC, BorderlineSMOTE, SMOTE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "class ImprovedCricketModel:\n",
    "    \"\"\"\n",
    "    Improved modeling pipeline with better handling of cricket object detection.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.models = {}\n",
    "        self.best_model = None\n",
    "        self.scaler = None\n",
    "        self.feature_selector = None\n",
    "        self.pca = None\n",
    "    \n",
    "    def load_and_prepare_data(self):\n",
    "        \"\"\"\n",
    "        Load data with better preprocessing.\n",
    "        \"\"\"\n",
    "        df = pd.read_csv(self.file_path)\n",
    "        df_clean = df.dropna()\n",
    "        \n",
    "        X = df_clean.drop(['image', 'tile_i', 'tile_j', 'tile_number', 'y'], \n",
    "                         axis=1, errors='ignore')\n",
    "        y = df_clean['y']\n",
    "        \n",
    "        print(f\"Original data shape: {X.shape}\")\n",
    "        print(f\"Class distribution:\\n{y.value_counts()}\")\n",
    "        \n",
    "        return X, y, df_clean\n",
    "    \n",
    "    def advanced_feature_selection(self, X, y, method='hybrid', n_features=None):\n",
    "        \"\"\"\n",
    "        Advanced feature selection combining multiple methods.\n",
    "        \"\"\"\n",
    "        if n_features is None:\n",
    "            n_features = min(300, X.shape[1])\n",
    "        \n",
    "        if method == 'univariate':\n",
    "            # Univariate feature selection\n",
    "            selector = SelectKBest(f_classif, k=n_features)\n",
    "            X_selected = selector.fit_transform(X, y)\n",
    "            print(f\"✓ Selected {n_features} features using univariate selection\")\n",
    "            self.feature_selector = selector\n",
    "            return X_selected\n",
    "        \n",
    "        elif method == 'rfe':\n",
    "            # Recursive Feature Elimination\n",
    "            estimator = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "            selector = RFE(estimator, n_features_to_select=n_features, step=50)\n",
    "            X_selected = selector.fit_transform(X, y)\n",
    "            print(f\"✓ Selected {n_features} features using RFE\")\n",
    "            self.feature_selector = selector\n",
    "            return X_selected\n",
    "        \n",
    "        elif method == 'hybrid':\n",
    "            # Combination: first univariate, then RFE\n",
    "            # Step 1: Univariate to reduce to 500\n",
    "            selector1 = SelectKBest(f_classif, k=min(500, X.shape[1]))\n",
    "            X_temp = selector1.fit_transform(X, y)\n",
    "            \n",
    "            # Step 2: RFE to final n_features\n",
    "            estimator = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "            selector2 = RFE(estimator, n_features_to_select=n_features, step=20)\n",
    "            X_selected = selector2.fit_transform(X_temp, y)\n",
    "            \n",
    "            print(f\"✓ Selected {n_features} features using hybrid selection\")\n",
    "            self.feature_selector = (selector1, selector2)\n",
    "            return X_selected\n",
    "        \n",
    "        else:\n",
    "            return X\n",
    "    \n",
    "    def improved_sampling(self, X, y, strategy='borderline_smote'):\n",
    "        \"\"\"\n",
    "        Improved sampling strategy for imbalanced data.\n",
    "        \"\"\"\n",
    "        print(f\"\\nOriginal distribution: {dict(zip(*np.unique(y, return_counts=True)))}\")\n",
    "        \n",
    "        # Random Undersampling of majority class (0)\n",
    "        rus = RandomUnderSampler(\n",
    "            sampling_strategy={0: len(y[y != 0])}, \n",
    "            random_state=42\n",
    "        )\n",
    "        X_rus, y_rus = rus.fit_resample(X, y)\n",
    "        print(f\"After undersampling: {dict(zip(*np.unique(y_rus, return_counts=True)))}\")\n",
    "        \n",
    "        # Borderline SMOTE (better than regular SMOTE for overlapping classes)\n",
    "        if strategy == 'borderline_smote':\n",
    "            oversample = BorderlineSMOTE(random_state=42, kind='borderline-2')\n",
    "        else:\n",
    "            oversample = SMOTE(random_state=42)\n",
    "        \n",
    "        X_balanced, y_balanced = oversample.fit_resample(X_rus, y_rus)\n",
    "        print(f\"After SMOTE: {dict(zip(*np.unique(y_balanced, return_counts=True)))}\")\n",
    "        \n",
    "        return X_balanced, y_balanced\n",
    "    \n",
    "    def create_ensemble_models(self):\n",
    "        \"\"\"\n",
    "        Create ensemble of diverse models.\n",
    "        \"\"\"\n",
    "        models = {\n",
    "            # Base models\n",
    "            'RandomForest': RandomForestClassifier(\n",
    "                n_estimators=200,\n",
    "                max_depth=20,\n",
    "                min_samples_split=10,\n",
    "                min_samples_leaf=4,\n",
    "                class_weight='balanced',\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            ),\n",
    "            \n",
    "            # 'GradientBoosting': GradientBoostingClassifier(\n",
    "            #     n_estimators=150,\n",
    "            #     max_depth=7,\n",
    "            #     learning_rate=0.1,\n",
    "            #     subsample=0.8,\n",
    "            #     random_state=42\n",
    "            # ),\n",
    "            \n",
    "            'SVC-RBF': SVC(\n",
    "                kernel='rbf',\n",
    "                C=10.0,\n",
    "                gamma='scale',\n",
    "                probability=True,\n",
    "                class_weight='balanced',\n",
    "                random_state=42\n",
    "            )\n",
    "            \n",
    "            # 'MLP': MLPClassifier(\n",
    "            #     hidden_layer_sizes=(128, 64, 32),\n",
    "            #     activation='relu',\n",
    "            #     solver='adam',\n",
    "            #     alpha=0.001,\n",
    "            #     batch_size=128,\n",
    "            #     learning_rate='adaptive',\n",
    "            #     max_iter=500,\n",
    "            #     early_stopping=True,\n",
    "            #     random_state=42\n",
    "            # )\n",
    "        }\n",
    "        \n",
    "        # Voting Classifier (Ensemble)\n",
    "        # voting_clf = VotingClassifier(\n",
    "        #     estimators=[\n",
    "        #         ('rf', models['RandomForest']),\n",
    "        #         ('gb', models['GradientBoosting']),\n",
    "        #         ('svc', models['SVC-RBF'])\n",
    "        #     ],\n",
    "        #     voting='soft'\n",
    "        # )\n",
    "        \n",
    "        # models['VotingEnsemble'] = voting_clf\n",
    "        \n",
    "        return models\n",
    "    \n",
    "    def train_with_cross_validation(self, X, y):\n",
    "        \"\"\"\n",
    "        Train models with cross-validation for better generalization.\n",
    "        \"\"\"\n",
    "        models = self.create_ensemble_models()\n",
    "        \n",
    "        # Stratified K-Fold\n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for model_name, model in models.items():\n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(f\"Training {model_name}...\")\n",
    "            print(f\"{'='*70}\")\n",
    "            \n",
    "            # Cross-validation scores\n",
    "            cv_scores = cross_val_score(model, X, y, cv=skf, \n",
    "                                       scoring='f1_weighted', n_jobs=-1)\n",
    "            \n",
    "            print(f\"CV F1 Scores: {cv_scores}\")\n",
    "            print(f\"Mean CV F1: {np.mean(cv_scores):.4f} (+/- {np.std(cv_scores):.4f})\")\n",
    "            \n",
    "            # Train on full training set\n",
    "            model.fit(X, y)\n",
    "            \n",
    "            results[model_name] = {\n",
    "                'model': model,\n",
    "                'cv_mean': np.mean(cv_scores),\n",
    "                'cv_std': np.std(cv_scores)\n",
    "            }\n",
    "        \n",
    "        # Select best model based on CV score\n",
    "        best_model_name = max(results, key=lambda x: results[x]['cv_mean'])\n",
    "        self.best_model = results[best_model_name]['model']\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Best Model: {best_model_name}\")\n",
    "        print(f\"CV F1 Score: {results[best_model_name]['cv_mean']:.4f}\")\n",
    "        print(f\"{'='*70}\\n\")\n",
    "        \n",
    "        return results, best_model_name\n",
    "    \n",
    "    def full_pipeline(self, use_pca=True, n_features=250):\n",
    "        \"\"\"\n",
    "        Complete training pipeline.\n",
    "        \"\"\"\n",
    "        # Load data\n",
    "        X, y, df_meta = self.load_and_prepare_data()\n",
    "        \n",
    "        # Feature selection\n",
    "        X_selected = self.advanced_feature_selection(X, y, method='hybrid', \n",
    "                                                     n_features=n_features)\n",
    "        \n",
    "        # Sampling\n",
    "        X_balanced, y_balanced = self.improved_sampling(X_selected, y, \n",
    "                                                        strategy='borderline_smote')\n",
    "        \n",
    "        # Scaling\n",
    "        self.scaler = RobustScaler()  # More robust to outliers than MinMaxScaler\n",
    "        X_scaled = self.scaler.fit_transform(X_balanced)\n",
    "        \n",
    "        # Optional PCA\n",
    "        if use_pca:\n",
    "            self.pca = PCA(n_components=0.95, random_state=42)\n",
    "            X_final = self.pca.fit_transform(X_scaled)\n",
    "            print(f\"PCA: {X_final.shape[1]} components (95% variance)\")\n",
    "        else:\n",
    "            X_final = X_scaled\n",
    "        \n",
    "        # Train and test split\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_final, y_balanced, test_size=0.2, \n",
    "            random_state=42, stratify=y_balanced\n",
    "        )\n",
    "        \n",
    "        # Train with cross-validation\n",
    "        results, best_model_name = self.train_with_cross_validation(X_train, y_train)\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        y_pred = self.best_model.predict(X_test)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"TEST SET EVALUATION\")\n",
    "        print(\"=\"*70)\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        \n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(cm)\n",
    "        \n",
    "        # Save models\n",
    "        self.save_models(best_model_name)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def save_models(self, best_model_name):\n",
    "        \"\"\"\n",
    "        Save trained models and preprocessors.\n",
    "        \"\"\"\n",
    "        with open('../models/best_model.pkl', 'wb') as f:\n",
    "            pickle.dump(self.best_model, f)\n",
    "        \n",
    "        with open('../models/scaler.pkl', 'wb') as f:\n",
    "            pickle.dump(self.scaler, f)\n",
    "        \n",
    "        if self.pca:\n",
    "            with open('../models/pca_model.pkl', 'wb') as f:\n",
    "                pickle.dump(self.pca, f)\n",
    "        \n",
    "        if self.feature_selector:\n",
    "            with open('../models/feature_selector.pkl', 'wb') as f:\n",
    "                pickle.dump(self.feature_selector, f)\n",
    "        \n",
    "        print(f\"\\n✓ Models saved (Best: {best_model_name})\")\n",
    "\n",
    "\n",
    "# Usage\n",
    "trainer = ImprovedCricketModel('enhanced_features_consolidation.csv')\n",
    "results = trainer.full_pipeline(use_pca=True, n_features=250)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
